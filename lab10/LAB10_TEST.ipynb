{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([2, 2], dtype=tf.int32, name=\"vector\")\n",
    "a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.constant([[0, 1], [2, 3]], name=\"b\")\n",
    "b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tf.zeros([2, 3], tf.int32) # [[0, 0, 0], [0, 0, 0]]\n",
    "c.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.],\n",
       "       [ 0.,  0.],\n",
       "       [ 0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor containing zeros, with shape and type as input_tensor\n",
    "input_tensor = tf.constant([[1,1], [2,2], [3,3]], dtype=tf.float32)\n",
    "e = tf.zeros_like(input_tensor)  #  [[0, 0], [0, 0], [0, 0]]\n",
    "e.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.],\n",
       "       [ 1.,  1.],\n",
       "       [ 1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = tf.ones_like(input_tensor) # [[1, 1], [1, 1], [1, 1]]\n",
    "f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create variable a with scalar value\n",
    "a = tf.Variable(2, name=\"scalar\")\n",
    "#create variable b as a vector\n",
    "b = tf.Variable([2, 3], name=\"vector\")\n",
    "#create variable c as a 2x2 matrix\n",
    "c = tf.Variable([[0, 1], [2, 3]], name=\"matrix\")\n",
    "# create variable W as 784 x 10 tensor, filled with zeros\n",
    "W = tf.Variable(tf.zeros([784,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign a * 2 to a and call that op a_times_two\n",
    "a = tf.Variable(2, name=\"scalar\")\n",
    "a_times_two = a.assign(a*2) # an operation that assigns value a*2 to a\n",
    "\n",
    "init = tf.global_variables_initializer() # an operation that initializes all variables\n",
    "sess.run(init) # run the init operation with session\n",
    "sess.run(a_times_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Const_2:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "node3 = tf.add(node1, node2) \n",
    "\n",
    "print(node1) \n",
    "print(node2)\n",
    "print(node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a directory to store our graph\n",
    "import os\n",
    "\n",
    "logs_dir = './graph'\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))\n",
    "print(sess.run(node3))\n",
    "sess.close() # close the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # write operations to the event file\n",
    "    writer = tf.summary.FileWriter(logs_dir, sess.graph) \n",
    "    print(sess.run([node1, node2]))\n",
    "    print(sess.run(node3))\n",
    "    # no need to write sess.close()\n",
    "  \n",
    "writer.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  7.  8.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create a placeholder of type float 32-bit, shape is a vector of 3 elements\n",
    "a = tf.placeholder(tf.float32, shape=[3])\n",
    "# create a constant of type float 32-bit, shape is a vector of 3 elements\n",
    "b = tf.constant([5, 5, 5], tf.float32)\n",
    "# use the placeholder as you would a constant or a variable\n",
    "c = a + b # Short for tf.add(a, b)\n",
    "with tf.Session() as sess:\n",
    "# feed [1, 2, 3] to placeholder a via the dict {a: [1, 2, 3]}\n",
    "# fetch value of c\n",
    "    print(sess.run(c, feed_dict={a: [1, 2, 3]}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"foo\"):\n",
    "    v = tf.get_variable(\"v\", [1])  # v.name == \"foo/v:0\"\n",
    "    w = tf.get_variable(\"w\", [1])  # w.name == \"foo/w:0\"\n",
    "with tf.variable_scope(\"foo\", reuse=True):\n",
    "    v1 = tf.get_variable(\"v\")  # The same as v above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -fs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from process_data import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "(64,)\n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "window_sz = 5\n",
    "batch_sz = 64\n",
    "index_words, dictionary, index_dictionary = process_data(vocab_size)\n",
    "batch_gen = get_batch_gen(index_words, window_sz, batch_sz)\n",
    "X, y = next(batch_gen)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( anarchism originated )\n",
      "( anarchism as )\n",
      "( anarchism a )\n",
      "( anarchism term )\n",
      "( originated anarchism )\n",
      "( originated as )\n",
      "( as anarchism )\n",
      "( as originated )\n",
      "( as a )\n",
      "( as term )\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): # print out the pairs\n",
    "    data = index_dictionary[X[i]]\n",
    "    label = index_dictionary[y[i,0]]\n",
    "    print('(', data, label,')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term of abuse first used against "
     ]
    }
   ],
   "source": [
    "for i in range(10): # print out the first 10 words in the text\n",
    "    print(index_dictionary[index_words[i]], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "dataset = tf.contrib.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(BATCH_SIZE) # stack BATCH_SIZE elements into one\n",
    "iterator = dataset.make_one_shot_iterator() # iterator\n",
    "next_batch = iterator.get_next() # an operation that gives the next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 1)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    data, label = sess.run(next_batch)\n",
    "    print(data.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramModel:\n",
    "  \"\"\" Build the graph for word2vec model \"\"\"\n",
    "  def __init__(self, hparams=None):\n",
    "\n",
    "    if hparams is None:\n",
    "        self.hps = get_default_hparams()\n",
    "    else:\n",
    "        self.hps = hparams\n",
    "\n",
    "    # define a variable to record training progress\n",
    "    self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    \n",
    "\n",
    "  def _create_input(self):\n",
    "    \"\"\" Step 1: define input and output \"\"\"\n",
    "\n",
    "    with tf.name_scope(\"data\"):\n",
    "      self.centers = tf.placeholder(tf.int32, [self.hps.num_pairs], name='centers')\n",
    "      self.targets = tf.placeholder(tf.int32, [self.hps.num_pairs, 1], name='targets')\n",
    "      dataset = tf.contrib.data.Dataset.from_tensor_slices((self.centers, self.targets))\n",
    "      dataset = dataset.repeat() # # Repeat the input indefinitely\n",
    "      dataset = dataset.batch(self.hps.batch_size)\n",
    "      \n",
    "        \n",
    "      self.iterator = dataset.make_initializable_iterator()  # create iterator\n",
    "      self.center_words, self.target_words = self.iterator.get_next()\n",
    "\n",
    "  def _create_embedding(self):\n",
    "    \"\"\" Step 2: define weights. \n",
    "        In word2vec, it's actually the weights that we care about\n",
    "    \"\"\"\n",
    "    with tf.device('/gpu:0'):\n",
    "      with tf.name_scope(\"embed\"):\n",
    "        self.embed_matrix = tf.Variable(\n",
    "                              tf.random_uniform([self.hps.vocab_size,\n",
    "                                                 self.hps.embed_size], -1.0, 1.0),\n",
    "                                                 name='embed_matrix')\n",
    "\n",
    "  def _create_loss(self):\n",
    "    \"\"\" Step 3 + 4: define the model + the loss function \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "      with tf.name_scope(\"loss\"):\n",
    "        # Step 3: define the inference\n",
    "        embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embed')\n",
    "\n",
    "\n",
    "        # Step 4: define loss function\n",
    "        # construct variables for NCE loss\n",
    "        nce_weight = tf.Variable(\n",
    "                        tf.truncated_normal([self.hps.vocab_size, self.hps.embed_size],\n",
    "                                            stddev=1.0 / (self.hps.embed_size ** 0.5)),\n",
    "                                            name='nce_weight')\n",
    "        nce_bias = tf.Variable(tf.zeros([self.hps.vocab_size]), name='nce_bias')\n",
    "\n",
    "        # define loss function to be NCE loss function\n",
    "        self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                  biases=nce_bias,\n",
    "                                                  labels=self.target_words,\n",
    "                                                  inputs=embed,\n",
    "                                                  num_sampled=self.hps.num_sampled,\n",
    "                                                  num_classes=self.hps.vocab_size), name='loss')\n",
    "  def _create_optimizer(self):\n",
    "    \"\"\" Step 5: define optimizer \"\"\"\n",
    "    with tf.device('/gpu:0'):\n",
    "      self.optimizer = tf.train.AdamOptimizer(self.hps.lr).minimize(self.loss,\n",
    "                                                         global_step=self.global_step)\n",
    "  \n",
    "  def _build_nearby_graph(self):\n",
    "    # Nodes for computing neighbors for a given word according to\n",
    "    # their cosine distance.\n",
    "    self.nearby_word = tf.placeholder(dtype=tf.int32)  # word id\n",
    "    nemb = tf.nn.l2_normalize(self.embed_matrix, 1)\n",
    "    nearby_emb = tf.gather(nemb, self.nearby_word)\n",
    "    nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=True)\n",
    "    self.nearby_val, self.nearby_idx = tf.nn.top_k(nearby_dist,\n",
    "                                         min(1000, self.hps.vocab_size))\n",
    "    \n",
    "\n",
    "  def _build_eval_graph(self):\n",
    "    \"\"\"Build the eval graph.\"\"\"\n",
    "    # Eval graph\n",
    "\n",
    "    # Each analogy task is to predict the 4th word (d) given three\n",
    "    # words: a, b, c.  E.g., a=italy, b=rome, c=france, we should\n",
    "    # predict d=paris.\n",
    "\n",
    "    # The eval feeds three vectors of word ids for a, b, c, each of\n",
    "    # which is of size N, where N is the number of analogies we want to\n",
    "    # evaluate in one batch.\n",
    "    self.analogy_a = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "    self.analogy_b = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "    self.analogy_c = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "\n",
    "    # Normalized word embeddings of shape [vocab_size, emb_dim].\n",
    "    nemb = tf.nn.l2_normalize(self.embed_matrix, 1)\n",
    "\n",
    "    # Each row of a_emb, b_emb, c_emb is a word's embedding vector.\n",
    "    # They all have the shape [N, emb_dim]\n",
    "    a_emb = tf.gather(nemb, self.analogy_a)  # a's embs\n",
    "    b_emb = tf.gather(nemb, self.analogy_b)  # b's embs\n",
    "    c_emb = tf.gather(nemb, self.analogy_c)  # c's embs\n",
    "\n",
    "    # We expect that d's embedding vectors on the unit hyper-sphere is\n",
    "    # near: c_emb + (b_emb - a_emb), which has the shape [N, emb_dim].\n",
    "    target = c_emb + (b_emb - a_emb)\n",
    "\n",
    "    # Compute cosine distance between each pair of target and vocab.\n",
    "    # dist has shape [N, vocab_size].\n",
    "    dist = tf.matmul(target, nemb, transpose_b=True)\n",
    "\n",
    "    # For each question (row in dist), find the top 20 words.\n",
    "    _, self.pred_idx = tf.nn.top_k(dist, 20)\n",
    "\n",
    "  def predict(self, sess, analogy):\n",
    "    \"\"\" Predict the top 20 answers for analogy questions \"\"\"\n",
    "    idx, = sess.run([self.pred_idx], {\n",
    "        self.analogy_a: analogy[:, 0],\n",
    "        self.analogy_b: analogy[:, 1],\n",
    "        self.analogy_c: analogy[:, 2]\n",
    "    })\n",
    "    return idx\n",
    "\n",
    "  def _create_summaries(self):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "      tf.summary.scalar(\"loss\", self.loss)\n",
    "      tf.summary.histogram(\"histogram_loss\", self.loss)\n",
    "      # because you have several summaries, we should merge them all\n",
    "      # into one op to make it easier to manage\n",
    "      self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "  def build_graph(self):\n",
    "    \"\"\" Build the graph for our model \"\"\"\n",
    "    self._create_input()\n",
    "    self._create_embedding()\n",
    "    self._create_loss()\n",
    "    self._create_optimizer()\n",
    "    self._build_eval_graph()\n",
    "    self._build_nearby_graph()\n",
    "    self._create_summaries()\n",
    "\n",
    "\n",
    "\n",
    "def train_model(sess, model, batch_gen, index_words, num_train_steps):\n",
    "  saver = tf.train.Saver()\n",
    "  # defaults to saving all variables - in this case embed_matrix, nce_weight, nce_bias\n",
    "\n",
    "  initial_step = 0\n",
    "  make_dir('checkpoints') # directory to store checkpoints\n",
    "\n",
    "\n",
    "  \n",
    "  sess.run(tf.global_variables_initializer()) # initialize all variables\n",
    "  ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "  # if that checkpoint exists, restore from checkpoint\n",
    "  if ckpt and ckpt.model_checkpoint_path:\n",
    "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "  total_loss = 0.0 # use this to calculate late average loss in the last SKIP_STEP steps\n",
    "  writer = tf.summary.FileWriter('graph/lr' + str(model.hps.lr), sess.graph)\n",
    "  initial_step = model.global_step.eval()\n",
    "  for index in range(initial_step, initial_step + num_train_steps):\n",
    "    # feed in new dataset  \n",
    "    if index % model.hps.new_dataset_every == 0:\n",
    "      try:\n",
    "          centers, targets = next(batch_gen)\n",
    "      except StopIteration: # generator has nothing left to generate\n",
    "          batch_gen = get_batch_gen(index_words, \n",
    "                                    model.hps.skip_window, \n",
    "                                    model.hps.num_pairs)\n",
    "          centers, targets = next(batch_gen)\n",
    "          print('Finished looking at the whole text')\n",
    "            \n",
    "      feed = {\n",
    "          model.centers: centers,\n",
    "          model.targets: targets\n",
    "      }\n",
    "      _ = sess.run(model.iterator.initializer, feed_dict = feed)\n",
    "      print('feeding in new dataset')\n",
    "      \n",
    "      \n",
    "    loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op])\n",
    "    writer.add_summary(summary, global_step=index)\n",
    "    total_loss += loss_batch\n",
    "    if (index + 1) % model.hps.skip_step == 0:\n",
    "        print('Average loss at step {}: {:5.1f}'.format(\n",
    "                                                  index,\n",
    "                                                  total_loss/model.hps.skip_step))\n",
    "        total_loss = 0.0\n",
    "        saver.save(sess, 'checkpoints/skip-gram', index)\n",
    "\n",
    "\n",
    "def get_default_hparams():\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        num_pairs = 10**6,                # number of (center, target) pairs \n",
    "                                          # in each dataset instance\n",
    "        vocab_size = 10000,\n",
    "        batch_size = 128,\n",
    "        embed_size = 300,                 # dimension of the word embedding vectors\n",
    "        skip_window = 3,                  # the context window\n",
    "        num_sampled = 100,                # number of negative examples to sample\n",
    "        lr = 0.005,                       # learning rate\n",
    "        new_dataset_every = 10**4,        # replace the original dataset every ? steps\n",
    "        num_train_steps = 2*10**5,        # number of training steps for each feed of dataset\n",
    "        skip_step = 2000\n",
    "    )\n",
    "    return hparams\n",
    "\n",
    "def main():\n",
    "\n",
    "  hps = get_default_hparams()\n",
    "  index_words, dictionary, index_dictionary = process_data(hps.vocab_size)\n",
    "  batch_gen = get_batch_gen(index_words, hps.skip_window, hps.num_pairs)\n",
    "                                                          \n",
    "  model = SkipGramModel(hparams = hps)\n",
    "  model.build_graph()\n",
    "  \n",
    "  \n",
    "  with tf.Session() as sess:\n",
    "    \n",
    "    # feed the model with dataset\n",
    "    centers, targets = next(batch_gen)\n",
    "    feed = {\n",
    "        model.centers: centers,\n",
    "        model.targets: targets\n",
    "    }\n",
    "    sess.run(model.iterator.initializer, feed_dict = feed) # initialize the iterator\n",
    "    train_model(sess, model, batch_gen, index_words, hps.num_train_steps)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "feeding in new dataset\n",
      "Average loss at step 1999:  76.0\n",
      "Average loss at step 3999:  17.0\n",
      "Average loss at step 5999:  10.3\n",
      "Average loss at step 7999:   8.7\n",
      "Average loss at step 9999:   6.7\n",
      "feeding in new dataset\n",
      "Average loss at step 11999:   8.3\n",
      "Average loss at step 13999:   7.5\n",
      "Average loss at step 15999:   7.5\n",
      "Average loss at step 17999:   7.1\n",
      "Average loss at step 19999:   6.1\n",
      "feeding in new dataset\n",
      "Average loss at step 21999:   7.2\n",
      "Average loss at step 23999:   6.9\n",
      "Average loss at step 25999:   6.7\n",
      "Average loss at step 27999:   6.3\n",
      "Average loss at step 29999:   6.1\n",
      "feeding in new dataset\n",
      "Average loss at step 31999:   6.4\n",
      "Average loss at step 33999:   6.3\n",
      "Average loss at step 35999:   6.5\n",
      "Average loss at step 37999:   6.6\n",
      "Average loss at step 39999:   5.6\n",
      "feeding in new dataset\n",
      "Average loss at step 41999:   6.8\n",
      "Average loss at step 43999:   6.6\n",
      "Average loss at step 45999:   6.4\n",
      "Average loss at step 47999:   6.4\n",
      "Average loss at step 49999:   5.9\n",
      "feeding in new dataset\n",
      "Average loss at step 51999:   6.5\n",
      "Average loss at step 53999:   6.5\n",
      "Average loss at step 55999:   6.4\n",
      "Average loss at step 57999:   6.4\n",
      "Average loss at step 59999:   5.8\n",
      "feeding in new dataset\n",
      "Average loss at step 61999:   6.5\n",
      "Average loss at step 63999:   6.2\n",
      "Average loss at step 65999:   6.6\n",
      "Average loss at step 67999:   6.5\n",
      "Average loss at step 69999:   5.8\n",
      "feeding in new dataset\n",
      "Average loss at step 71999:   6.6\n",
      "Average loss at step 73999:   6.4\n",
      "Average loss at step 75999:   6.5\n",
      "Average loss at step 77999:   6.5\n",
      "Average loss at step 79999:   5.8\n",
      "feeding in new dataset\n",
      "Average loss at step 81999:   6.5\n",
      "Average loss at step 83999:   6.4\n",
      "Average loss at step 85999:   6.3\n",
      "Average loss at step 87999:   6.0\n",
      "Average loss at step 89999:   5.7\n",
      "feeding in new dataset\n",
      "Average loss at step 91999:   6.5\n",
      "Average loss at step 93999:   6.5\n",
      "Average loss at step 95999:   6.4\n",
      "Average loss at step 97999:   6.4\n",
      "Average loss at step 99999:   5.7\n",
      "feeding in new dataset\n",
      "Average loss at step 101999:   6.4\n",
      "Average loss at step 103999:   6.4\n",
      "Average loss at step 105999:   6.5\n",
      "Average loss at step 107999:   6.3\n",
      "Average loss at step 109999:   5.7\n",
      "feeding in new dataset\n",
      "Average loss at step 111999:   6.5\n",
      "Average loss at step 113999:   6.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-dd9a1f3548c4>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    226\u001b[0m     }\n\u001b[0;32m    227\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# initialize the iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m     \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-dd9a1f3548c4>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(sess, model, batch_gen, index_words, num_train_steps)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[0mloss_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "hps = get_default_hparams()\n",
    "# get dictionary \n",
    "index_words, dictionary, index_dictionary = process_data(hps.vocab_size)\n",
    "\n",
    "# build model\n",
    "model = SkipGramModel(hps)\n",
    "model.build_graph()\n",
    "\n",
    "# initialize variables and restore checkpoint\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearby(words, model, sess, dictionary, index_dictionary, num=20):\n",
    "    \"\"\"Prints out nearby words given a list of words.\"\"\"\n",
    "    ids = np.array([dictionary.get(x, 0) for x in words])\n",
    "    vals, idx = sess.run(\n",
    "        [model.nearby_val, model.nearby_idx], {model.nearby_word: ids})\n",
    "    for i in range(len(words)):\n",
    "      print(\"\\n%s\\n=====================================\" % (words[i]))\n",
    "      for (neighbor, distance) in zip(idx[i, :num], vals[i, :num]):\n",
    "        print(\"%-20s %6.4f\" % (index_dictionary.get(neighbor), distance))\n",
    "        \n",
    "def analogy(line, model, sess, dictionary, index_dictionary):\n",
    "  \"\"\" Prints the top k anologies for a given array which contain 3 words\"\"\"\n",
    "  analogy = np.array([dictionary.get(w, 0) for w in line])[np.newaxis,:]\n",
    "  idx = model.predict(sess, analogy)\n",
    "  print(line)\n",
    "  for i in idx[0]:\n",
    "    print(index_dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['machine', 'learning']\n",
    "nearby(words, model, sess, dictionary, index_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy(['london', 'england', 'berlin'], model, sess, dictionary, index_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = 300\n",
    "\n",
    "embed_matrix = sess.run(model.embed_matrix) # get the embed matrix\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(embed_matrix[:rng])\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "for i in range(rng):\n",
    "  plt.scatter(X_embedded[i][0], X_embedded[i][1])\n",
    "  plt.text(X_embedded[i][0]+0.2,\n",
    "           X_embedded[i][1]+0.2,\n",
    "           index_dictionary.get(i, 0), fontsize=18)\n",
    "  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
